import { type Node, type Edge } from '@xyflow/react';

// å‡ºåŠ›ã—ãŸã„JSONã®å‹å®šç¾©
type PipelineStep = {
    id: string;
    type: string;
    label: string;
    config: Record<string, any>;
    next_steps: string[]; // æ¥ç¶šå…ˆã®IDãƒªã‚¹ãƒˆ
};

type PipelineConfig = {
    version: string;
    generated_at: string;
    pipeline: PipelineStep[];
};

export const generatePipelineConfig = (nodes: Node[], edges: Edge[]): PipelineConfig => {
    // ãƒãƒ¼ãƒ‰ã”ã¨ã«ã€Œæ¬¡ã®æ¥ç¶šå…ˆã€ã‚’æ¢ã—ã‚„ã™ã„ã‚ˆã†ã«ãƒãƒƒãƒ—åŒ–
    // (ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã‚’èµ°æŸ»ã—ã¦ã€source -> target ã®é–¢ä¿‚ã‚’æ•´ç†)
    const adjacencyList: Record<string, string[]> = {};

    edges.forEach((edge) => {
        if (!adjacencyList[edge.source]) {
            adjacencyList[edge.source] = [];
        }
        adjacencyList[edge.source].push(edge.target);
    });

    // ãƒãƒ¼ãƒ‰ã‚’å¤‰æ›
    const steps: PipelineStep[] = nodes.map((node) => {
        // ã“ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰å‡ºã¦ã„ã‚‹ã‚¨ãƒƒã‚¸ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆIDã‚’å–å¾—
        const nextNodeIds = adjacencyList[node.id] || [];

        return {
            id: node.id,
            type: node.data.type as string, // 'processing', 'trigger' ãªã©
            label: node.data.label as string,
            config: node.data.config as Record<string, any> || {},
            next_steps: nextNodeIds, // ã“ã‚Œã§ã€Œã¤ãªãè¾¼ã¿ã€ãŒè¡¨ç¾ã§ãã‚‹
        };
    });

    // é–‹å§‹ãƒãƒ¼ãƒ‰ï¼ˆTriggerï¼‰ãŒå…ˆé ­ã«æ¥ã‚‹ã‚ˆã†ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‚‚ã„ã„ã§ã™ãŒã€
    // ä¸€æ—¦ã¯ãã®ã¾ã¾ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã—ã¾ã™
    return {
        version: "1.0.0",
        generated_at: new Date().toISOString(),
        pipeline: steps,
    };
};

// ...æ—¢å­˜ã® generatePipelineConfig ...

// â˜…è¿½åŠ : Pythonã‚³ãƒ¼ãƒ‰ç”Ÿæˆé–¢æ•°
export const generatePythonCode = (nodes: any[], edges: any[]): string => {
    const config = generatePipelineConfig(nodes, edges);
    const steps = config.pipeline;

    let code = `# Generated by NeuroFlow Architect\n`;
    code += `# Date: ${new Date().toISOString()}\n\n`;
    code += `import os\n`;
    code += `from typing import Any, Dict\n\n`;
    code += `class NeuroPipeline:\n`;
    code += `    def __init__(self):\n`;
    code += `        print("Initializing pipeline...")\n`;

    // å„ãƒãƒ¼ãƒ‰ã®è¨­å®šã‚’ __init__ ã«æ›¸ãå‡ºã™
    steps.forEach(step => {
        if (step.type === 'processing' && step.label.includes('LLM')) {
            code += `        self.llm = "ChatOpenAI(model='${step.config.model || 'gpt-4'}', temp=${step.config.temperature || 0.7})"\n`;
        }
        if (step.type === 'data') {
            code += `        self.vector_db = "Pinecone(index='${step.config.index || 'default'}')"\n`;
        }
    });

    code += `\n    def run(self, input_data: str) -> Dict[str, Any]:\n`;
    code += `        context = {}\n`;
    code += `        current_data = input_data\n\n`;

    // å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã‚’æ›¸ãå‡ºã™
    steps.forEach((step, index) => {
        code += `        # Step ${index + 1}: ${step.label} (${step.type})\n`;

        if (step.type === 'trigger') {
            code += `        print(f"ğŸš€ Trigger: {current_data}")\n`;
        }
        else if (step.type === 'processing') {
            if (step.label.includes('Splitter')) {
                code += `        current_data = self._split_text(current_data, chunk_size=${step.config.chunkSize || 1000})\n`;
            } else if (step.label.includes('LLM')) {
                code += `        current_data = self._call_llm(current_data)\n`;
            } else {
                code += `        # Processing logic for ${step.label}...\n`;
            }
        }
        else if (step.type === 'data') {
            code += `        context['retrieved'] = self._search_db(current_data)\n`;
        }
        else if (step.type === 'action') {
            code += `        print(f"ğŸ“§ Sending output: {current_data}")\n`;
        }

        code += `\n`;
    });

    code += `        return {"status": "success", "output": current_data}\n\n`;

    // ãƒ€ãƒŸãƒ¼ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    code += `    def _split_text(self, text, chunk_size): return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n`;
    code += `    def _call_llm(self, prompt): return "LLM Response"\n`;
    code += `    def _search_db(self, query): return ["doc1", "doc2"]\n`;

    return code;
};